{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open('cercles.txt','r'))\n",
    "x = np.matrix(data[:,:-1]).T # here x is row vector, n*d, so w1x+b （dh*d*d*1+dh*1）-->> x*w1T+b(1*d*d*dh+1*dh)\n",
    "y = np.matrix(data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1100)\n",
      "(2, 1)\n",
      "(1, 1100)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x))\n",
    "print(np.shape(x[:,0]))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ha=W1*x+b1, dh-dimension \n",
    "hs=rect(ha),\n",
    "oa=W2*hs+b2, m-dimension\n",
    "os=softmax(oa)\n",
    "loss=-log(os)\n",
    "Finite difference gradient check：derivation(parameter)=(f(a+h)-f(a))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. As a beginning, start with an implementation that computes the gradients for a single example, and check that the gradient is correct using the finite difference method described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot, rect, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(m,y): # y can not be a number\n",
    "    result = []\n",
    "    y = y.T\n",
    "    for i in range(len(y)):\n",
    "        result.append(np.eye(m)[int(y[i])])\n",
    "    result = np.matrix(result).T\n",
    "    return result\n",
    "\n",
    "def rect(x): # x must be a matrix\n",
    "    result=[]\n",
    "    for i in range(np.shape(x)[1]):\n",
    "        temp=[]\n",
    "        for j in range(np.shape(x)[0]):\n",
    "            temp.append(max(0,x[j,i]))\n",
    "        result.append(temp)\n",
    "    result = np.matrix(result).T\n",
    "    return result\n",
    "\n",
    "def softmax(x): # x must be a matrix\n",
    "    result = []\n",
    "    m,n = np.shape(x)\n",
    "    for i in range(n):\n",
    "        temp = []\n",
    "        sum = np.sum(np.exp(x[:,i]))\n",
    "#        print(sum)\n",
    "        for j in range(m):\n",
    "            temp.append(np.exp(x[j,i])/sum)\n",
    "        result.append(temp)\n",
    "    result = np.matrix(result).T\n",
    "    return result \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[-0.67578067 -0.25028273]\n",
      " [ 0.58216531 -0.52019275]]\n",
      "w2:\n",
      " [[-0.10792255 -0.04123022]\n",
      " [ 0.03137159 -0.23308715]]\n"
     ]
    }
   ],
   "source": [
    "#initial the parameters\n",
    "\n",
    "dh = 2\n",
    "d = 2\n",
    "m = 2\n",
    "\n",
    "w1 = np.matrix((np.random.uniform(-1/d**0.5,1/d**0.5,d*dh)).reshape(dh,d))\n",
    "print('w1:\\n',w1)\n",
    "b1 = np.matrix([0]*dh).T\n",
    "w2 = np.matrix((np.random.uniform(-1/dh*0.5,1/dh*0.5,dh*m)).reshape(m,dh))\n",
    "print('w2:\\n',w2)\n",
    "b2 = np.matrix([0]*m).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate x,ha,hs,oa,os\n",
    "\n",
    "def fprop(x,y,w1,b1,w2,b2):\n",
    "#    mean = np.mean(x)\n",
    "#    s = np.std(x)\n",
    "#    x = (x-mean)/s\n",
    "    ha = w1*x+b1\n",
    "#    print(w1,'\\n',x[:,0])\n",
    "#    print('ha:\\n',np.shape(ha))\n",
    "    hs = rect(ha)\n",
    "#    print('hs:\\n',np.shape(hs))\n",
    "    oa = w2*hs+b2\n",
    "#    print('oa:\\n',np.shape(oa))\n",
    "    os = softmax(oa)\n",
    "#    print('os:\\n',np.shape(os))\n",
    "    L = -np.log(os[int(y)])\n",
    "    return(ha,hs,oa,os,L)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ha: [[-0.57558934]\n",
      " [ 0.26053306]] \n",
      "hs: [[0.        ]\n",
      " [0.26053306]] \n",
      "oa: [[-0.01074184]\n",
      " [-0.06072691]] \n",
      "os: [[0.51249367]\n",
      " [0.48750633]] \n",
      "label: 1.0 \n",
      "L: [[0.718452]]\n"
     ]
    }
   ],
   "source": [
    "ha,hs,oa,os,L=fprop(x[:,0],y[0,0],w1,b1,w2,b2)\n",
    "print('ha:',ha,'\\nhs:',hs,'\\noa:',oa,'\\nos:',os,'\\nlabel:',y[0,0],'\\nL:',L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bprop(m,x,y,w1,b1,w2,b2):\n",
    "    ha = fprop(x,y,w1,b1,w2,b2)[0]\n",
    "    hs = fprop(x,y,w1,b1,w2,b2)[1]\n",
    "    os = fprop(x,y,w1,b1,w2,b2)[3]\n",
    "    grad_oa = os-onehot(m,y)\n",
    "    grad_w2 = grad_oa*hs.T\n",
    "    grad_b2 = grad_oa\n",
    "    grad_hs = w2.T*grad_oa\n",
    "    grad_ha=[]\n",
    "    for i in range(dh):\n",
    "        if (ha[i,0] <= 0) : grad_ha.append(0)\n",
    "        else: grad_ha.append(grad_hs[i,0])\n",
    "    grad_ha = np.matrix(grad_ha).T\n",
    "    grad_w1 = grad_ha*x.T \n",
    "    grad_b1 = grad_ha\n",
    "    return(grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1, grad_b1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goa: [[ 0.51249367]\n",
      " [-0.51249367]] \n",
      "gw2: [[ 0.          0.13352154]\n",
      " [-0.         -0.13352154]] \n",
      "gb2: [[ 0.51249367]\n",
      " [-0.51249367]] \n",
      "gw1: [[0.         0.        ]\n",
      " [0.0721014  0.03144585]] \n",
      "gb1: [[0.        ]\n",
      " [0.09832546]] \n",
      "gha: [[0.        ]\n",
      " [0.09832546]] \n",
      "ghs: [[-0.07138737]\n",
      " [ 0.09832546]]\n"
     ]
    }
   ],
   "source": [
    "[goa,gw2,gb2,ghs,gha,gw1,gb1] = bprop(m,x[:,0],np.matrix(y[0,0]),w1,b1,w2,b2)\n",
    "print('goa:',goa,'\\ngw2:',gw2,'\\ngb2:',gb2,'\\ngw1:',gw1,'\\ngb1:',gb1,'\\ngha:',gha,'\\nghs:',ghs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Display the gradients for both methods (direct computation and finite difference) for a small network (e.g. d = 2 and dh = 2) with random weights and for a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[0,0] \n",
    "# dL/d(w2[0,0])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[1][0,0]\n",
    "# (fprop(x[:,0],y[0,0],w1,b1,w2',b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b22))/epsilon where w2'=w2+[[epsilon,0],[0,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w2_new = np.zeros([2,2])\n",
    "w2_new[0,0] = epsilon\n",
    "w2_new += w2\n",
    "diff_gw2 = (fprop(x[:,0],y[0,0],w1,b1,w2_new,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_gw2 == gw2[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000064]]\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[0,1] \n",
    "# dL/d(w2[0,1])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[1][0,1]\n",
    "# (fprop(x[:,0],y[0,0],w1,b1,w2',b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where w2'=w2+[[0,epsilon],[0,0]]\n",
    "w2_new = np.zeros([2,2])\n",
    "w2_new[0,1] = epsilon\n",
    "w2_new += w2\n",
    "diff_gw2 = (fprop(x[:,0],y[0,0],w1,b1,w2_new,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_gw2/gw2[0,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[1,0] \n",
    "# dL/d(w2[1,0])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[1][1,0]\n",
    "# (fprop(x[:,0],y[0,0],w1,b1,w2',b2)-fprop(x,w1,b1,w2,b2))/epsilon where w2'=w2+[[0,0],[epsilon,0]]\n",
    "w2_new = np.zeros([2,2])\n",
    "w2_new[1,0] = epsilon\n",
    "w2_new += w2\n",
    "diff_gw2 = (fprop(x[:,0],y[0,0],w1,b1,w2_new,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_gw2 == gw2[1,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999937]]\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[1,1] \n",
    "# dL/d(w2[1,1])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[1][1,1]\n",
    "# (fprop(x[:,0],y[0,0],w1,b1,w2',b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where w2'=w2+[[0,0],[0,epsilon]]\n",
    "w2_new = np.zeros([2,2])\n",
    "w2_new[1,1] = epsilon\n",
    "w2_new += w2\n",
    "diff_gw2 = (fprop(x[:,0],y[0,0],w1,b1,w2_new,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_gw2 / gw2[1,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000244]]\n"
     ]
    }
   ],
   "source": [
    "# verify for b2[0,0] \n",
    "# dL/d(b2[0,0])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[2][0,0]\n",
    "# (fprop(x[:,0],y[0,0],w1,b1,w2,b2')-fprop(x,w1,b1,w2,b2))/epsilon where b2'=b2+[epsilon,0]\n",
    "b2_new = np.zeros([1,2]).T\n",
    "b2_new[0,0] = epsilon\n",
    "b2_new += b2\n",
    "diff_b2 = (fprop(x[:,0],y[0,0],w1,b1,w2,b2_new)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_b2 / gb2[0,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999756]]\n"
     ]
    }
   ],
   "source": [
    "# verify for b2[1,0] \n",
    "# dL/d(b2[1,0])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[2][1,0]\n",
    "# (fprop(x[:,0],y[0,0],w1,b1,w2,b2')-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where b2'=b2+[0,epsilon]\n",
    "b2_new = np.zeros([1,2]).T\n",
    "b2_new[1,0] = epsilon\n",
    "b2_new += b2\n",
    "diff_b2 = (fprop(x[:,0],y[0,0],w1,b1,w2,b2_new)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_b2 / gb2[1,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "# verify for w1[0,0] \n",
    "# dL/d(w2[0,0])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[3][0,0]\n",
    "# (fprop(x[:,0],y[0,0],w1',b1,w2,b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where w1'=w1+[[epsilon,0],[0,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w1_new = np.zeros([2,2])\n",
    "w1_new[0,0] = epsilon\n",
    "w1_new += w1\n",
    "diff_gw1 = (fprop(x[:,0],y[0,0],w1_new,b1,w2,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_gw1 == gw1[0,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "# verify for w1[0,1] \n",
    "# dL/d(w1[0,1])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[3][0,1]\n",
    "# (fprop(x[:,0],y[0,0],w1',b1,w2,b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where w1'=w1+[[0,epsilon],[0,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w1_new = np.zeros([2,2])\n",
    "w1_new[0,1] = epsilon\n",
    "w1_new += w1\n",
    "diff_gw1 = (fprop(x[:,0],y[0,0],w1_new,b1,w2,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_gw1 == gw1[0,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000034]]\n"
     ]
    }
   ],
   "source": [
    "# verify for w1[1,0] \n",
    "# dL/d(w1[1,0])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[3][1,0]\n",
    "# (fprop(x[:,0],y[0,0],w1',b1,w2,b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where w1'=w1+[[0,0],[epsilon,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w1_new = np.zeros([2,2])\n",
    "w1_new[1,0] = epsilon\n",
    "w1_new += w1\n",
    "diff_gw1 = (fprop(x[:,0],y[0,0],w1_new,b1,w2,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_gw1 / gw1[1,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000015]]\n"
     ]
    }
   ],
   "source": [
    "# verify for w1[1,1] \n",
    "# dL/d(w1[1,1])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[3][1,1]\n",
    "# (fprop(x[:,0],y[0,0],w1',b1,w2,b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where w1'=w1+[[0,0],[0,epsilon]]\n",
    "epsilon = (10)**(-5)\n",
    "w1_new = np.zeros([2,2])\n",
    "w1_new[1,1] = epsilon\n",
    "w1_new += w1\n",
    "diff_gw1 = (fprop(x[:,0],y[0,0],w1_new,b1,w2,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_gw1 / gw1[1,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "# verify for b1[0,0] \n",
    "# dL/d(b1[0,0])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[4][0,0]\n",
    "# (fprop(x[:,0],y[0,0],w1,b1',w2,b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where b1'=b1+[epsilon,0]\n",
    "b1_new = np.zeros([1,2]).T\n",
    "b1_new[0,0] = epsilon\n",
    "b1_new += b1\n",
    "diff_b1 = (fprop(x[:,0],y[0,0],w1,b1_new,w2,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_b1 == gb1[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000047]]\n"
     ]
    }
   ],
   "source": [
    "# verify for b1[1,0] \n",
    "# dL/d(b1[0,0])=bprop(m,x[:,0],np.matrix(y[0,0]),os)[4][1,0]\n",
    "# (fprop(x[:,0],y[0,0],w1,b1',w2,b2)-fprop(x[:,0],y[0,0],w1,b1,w2,b2))/epsilon where b1'=b1+[0,epsilon]\n",
    "b1_new = np.zeros([1,2]).T\n",
    "b1_new[1,0] = epsilon\n",
    "b1_new += b1\n",
    "diff_b1 = (fprop(x[:,0],y[0,0],w1,b1_new,w2,b2)[-1]-fprop(x[:,0],y[0,0],w1,b1,w2,b2)[-1])/epsilon\n",
    "print (diff_b1 / gb1[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add a hyperparameter for the minibatch size K to allow compute the gradients on a minibatch of K examples (in a matrix), by looping over the K examples (this is a small addition to your previous code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# fprop in part3.1\n",
    "\n",
    "def fprop(x,y,w1,b1,w2,b2):\n",
    "\n",
    "    ha = w1*x+b1\n",
    "    hs = rect(ha)\n",
    "    oa = w2*hs+b2\n",
    "    os = softmax(oa)\n",
    "    L = -np.log(os[int(y)])\n",
    "    return(ha,hs,oa,os,L)\n",
    "'''\n",
    "\n",
    "def fprop_k_loop(x,y,w1,b1,w2,b2,k):\n",
    "    sum_L = 0\n",
    "    for i in k:\n",
    "        sum_L += fprop(x[:,i],y[0,i],w1,b1,w2,b2)[-1]  \n",
    "    return np.linalg.det(sum_L/len(k))\n",
    "\n",
    "'''\n",
    "# bprop in part3.1\n",
    "\n",
    "def bprop(m,x,y,w1,b1,w2,b2):\n",
    "    ha = fprop(x,y,w1,b1,w2,b2)[0]\n",
    "    hs = fprop(x,y,w1,b1,w2,b2)[1]\n",
    "    os = fprop(x,y,w1,b1,w2,b2)[3]\n",
    "    grad_oa = os-onehot(m,y)\n",
    "    grad_w2 = grad_oa*hs.T\n",
    "    grad_b2 = grad_oa\n",
    "    grad_hs = w2.T*grad_oa\n",
    "    grad_ha=[]\n",
    "    for i in range(dh):\n",
    "        if (ha[i,0] <= 0) : grad_ha.append(0)\n",
    "        else: grad_ha.append(grad_hs[i,0])\n",
    "    grad_ha = np.matrix(grad_ha).T\n",
    "    grad_w1 = grad_ha*x.T \n",
    "    grad_b1 = grad_ha\n",
    "    return(grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1, grad_b1)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def bprop_k_loop(m,x,y,w1,b1,w2,b2,k,dh):\n",
    "    grad_oa = np.zeros([m,1])\n",
    "    grad_w2 = np.zeros([m,dh])\n",
    "    grad_b2 = np.zeros([m,1])\n",
    "    grad_hs = np.zeros([dh,1])\n",
    "    grad_ha = np.zeros([dh,1])\n",
    "    grad_w1 = np.zeros([dh,d])\n",
    "    grad_b1 = np.zeros([dh,1])\n",
    "    for i in k :\n",
    "        [grad_oai, grad_w2i, grad_b2i, grad_hsi, grad_hai, grad_w1i, grad_b1i]= bprop(m,x[:,i],np.matrix(y[0,i]),w1,b1,w2,b2)\n",
    "        grad_oa += grad_oai\n",
    "        grad_w2 += grad_w2i\n",
    "        grad_b2 += grad_b2i\n",
    "        grad_hs += grad_hsi\n",
    "        grad_ha += grad_hai\n",
    "        grad_w1 += grad_w1i\n",
    "        grad_b1 += grad_b1i       \n",
    "    grad_oa /= len(k)\n",
    "    grad_w2 /= len(k)\n",
    "    grad_b2 /= len(k)\n",
    "    grad_hs /= len(k)\n",
    "    grad_ha /= len(k)\n",
    "    grad_w1 /= len(k)\n",
    "    grad_b1 /= len(k)\n",
    "    return (grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1,grad_b1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display the gradients for both methods (direct computation and finite difference) for a small network (e.g. d = 2 and dh = 2) with random weights and for a minibatch with 10 examples (you can use examples from both classes from the two circles dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_oa:\n",
      " [[-0.19718862]\n",
      " [ 0.19718862]] \n",
      "grad_w2:\n",
      " [[-0.09208699 -0.0628827 ]\n",
      " [ 0.09208699  0.0628827 ]] \n",
      "grad_b2:\n",
      " [[-0.19718862]\n",
      " [ 0.19718862]] \n",
      "grad_w1:\n",
      " [[-0.01890475 -0.00020662]\n",
      " [-0.02551785 -0.00536556]] \n",
      "grad_b1:\n",
      " [[ 0.02175094]\n",
      " [-0.02705901]]\n"
     ]
    }
   ],
   "source": [
    "# verify for each parameter on a minibatch k = [0,1,2,...,10]\n",
    "k=np.arange(10)\n",
    "grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1,grad_b1 = bprop_k_loop(m,x,y,w1,b1,w2,b2,k,dh)\n",
    "print('grad_oa:\\n',grad_oa,'\\ngrad_w2:\\n',grad_w2,'\\ngrad_b2:\\n',grad_b2,'\\ngrad_w1:\\n',grad_w1,'\\ngrad_b1:\\n',grad_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999985538216867\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[0,0]\n",
    "# dL/d(w2[0,0])= grad_w2[0,0] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1,w2',b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where w2'=w2+[[epsilon,0][0,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w2_new = np.zeros([2,2])\n",
    "w2_new[0,0] = epsilon\n",
    "w2_new += w2\n",
    "diff_gw2 = (fprop_k_loop(x,y,w1,b1,w2_new,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_gw2 / grad_w2[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999981057717863\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[0,1]\n",
    "# dL/d(w2[0,1])= grad_w2[0,1] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1,w2',b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where w2'=w2+[[0,epsilon],[0,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w2_new = np.zeros([2,2])\n",
    "w2_new[0,1] = epsilon\n",
    "w2_new += w2\n",
    "diff_gw2 = (fprop_k_loop(x,y,w1,b1,w2_new,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_gw2 / grad_w2[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000014461139695\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[1,0]\n",
    "# dL/d(w2[1,0])= grad_w2[1,0] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1,w2',b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where w2'=w2+[[0,0],[epsilon,0]\n",
    "epsilon = (10)**(-5)\n",
    "w2_new = np.zeros([2,2])\n",
    "w2_new[1,0] = epsilon\n",
    "w2_new += w2\n",
    "diff_gw2 = (fprop_k_loop(x,y,w1,b1,w2_new,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_gw2 / grad_w2[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000001894280663\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[1,1]\n",
    "# dL/d(w2[1,1])= grad_w2[0,1] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1,w2',b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where w2'=w2+[[0,0],[0,epsilon]]\n",
    "epsilon = (10)**(-5)\n",
    "w2_new = np.zeros([2,2])\n",
    "w2_new[1,1] = epsilon\n",
    "w2_new += w2\n",
    "diff_gw2 = (fprop_k_loop(x,y,w1,b1,w2_new,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_gw2 / grad_w2[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999936697076376\n"
     ]
    }
   ],
   "source": [
    "# verify for b2[0,0]\n",
    "# dL/d(b2[0,0])= grad_b2[0,0] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1,w2,b2',k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where b2'=b2+[[epsilon],[0]]\n",
    "epsilon = (10)**(-5)\n",
    "b2_new = np.zeros([2,1])\n",
    "b2_new[0,0] = epsilon\n",
    "b2_new += b2\n",
    "diff_b2 = (fprop_k_loop(x,y,w1,b1,w2,b2_new,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_b2 / grad_b2[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000063303590017\n"
     ]
    }
   ],
   "source": [
    "# verify for b2[1,0]\n",
    "# dL/d(b2[1,0])= grad_b2[1,0] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1,w2,b2',k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where b2'=b2+[[0],[epsilon]]\n",
    "epsilon = (10)**(-5)\n",
    "b2_new = np.zeros([2,1])\n",
    "b2_new[1,0] = epsilon\n",
    "b2_new += b2\n",
    "diff_b2 = (fprop_k_loop(x,y,w1,b1,w2,b2_new,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_b2 / grad_b2[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999997059476512\n"
     ]
    }
   ],
   "source": [
    "# verify for w1[0,0]\n",
    "# dL/d(w1[0,0])= grad_w1[0,0] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1',b1,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where w1'=w1+[[epsilon,0],[0,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w1_new = np.zeros([2,2])\n",
    "w1_new[0,0] = epsilon\n",
    "w1_new += w1\n",
    "diff_gw1 = (fprop_k_loop(x,y,w1_new,b1,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_gw1 / grad_w1[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999916654829426\n"
     ]
    }
   ],
   "source": [
    "# verify for w1[0,1]\n",
    "# dL/d(w1[0,1])= grad_w1[0,1] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1',b1,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where w1'=w1+[[0,epsilon],[0,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w1_new = np.zeros([2,2])\n",
    "w1_new[0,1] = epsilon\n",
    "w1_new += w1\n",
    "diff_gw1 = (fprop_k_loop(x,y,w1_new,b1,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_gw1 / grad_w1[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999993291806428\n"
     ]
    }
   ],
   "source": [
    "# verify for w1[1,0]\n",
    "# dL/d(w1[1,0])= grad_w1[1,0] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1',b1,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where w1'=w1+[[0,0][epsilon,0]]\n",
    "epsilon = (10)**(-5)\n",
    "w1_new = np.zeros([2,2])\n",
    "w1_new[1,0] = epsilon\n",
    "w1_new += w1\n",
    "diff_gw1 = (fprop_k_loop(x,y,w1_new,b1,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_gw1 / grad_w1[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999992191666146\n"
     ]
    }
   ],
   "source": [
    "# verify for w2[1,1]\n",
    "# dL/d(w2[1,1])= grad_w2[1,1] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1,w2',b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where w2'=w2+[[0,0][0,epsilon]]\n",
    "epsilon = (10)**(-5)\n",
    "w1_new = np.zeros([2,2])\n",
    "w1_new[1,1] = epsilon\n",
    "w1_new += w1\n",
    "diff_gw1 = (fprop_k_loop(x,y,w1_new,b1,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_gw1 / grad_w1[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000003333662255\n"
     ]
    }
   ],
   "source": [
    "# verify for b1[0,0]\n",
    "# dL/d(b1[0,0])= grad_b1[0,0] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1',w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where b1'=b1+[[epsilon],[0]]\n",
    "epsilon = (10)**(-5)\n",
    "b1_new = np.zeros([2,1])\n",
    "b1_new[0,0] = epsilon\n",
    "b1_new += b1\n",
    "diff_b1 = (fprop_k_loop(x,y,w1,b1_new,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_b1 / grad_b1[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999991512086401\n"
     ]
    }
   ],
   "source": [
    "# verify for b1[1,0]\n",
    "# dL/d(b1[1,0])= grad_b1[1,0] we have got it in part3.3\n",
    "# (fprop_k_loop(x,y,w1,b1',w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon where b1'=b1+[[0],[epsilon]]\n",
    "epsilon = (10)**(-5)\n",
    "b1_new = np.zeros([2,1])\n",
    "b1_new[1,0] = epsilon\n",
    "b1_new += b1\n",
    "diff_b1 = (fprop_k_loop(x,y,w1,b1_new,w2,b2,k)-fprop_k_loop(x,y,w1,b1,w2,b2,k))/epsilon\n",
    "print (diff_b1 / grad_b1[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train your neural network using gradient descent on the two circles dataset. Plot the decision regions for several different values of the hyperparameters (weight decay, number of hidden units, early stopping) so as to illustrate their effect on the capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.shape(x)[1]\n",
    "inds = np.arange(n)\n",
    "np.random.shuffle(inds)\n",
    "train_inds = inds[:800]\n",
    "test_inds = inds[800:]\n",
    "x_train = x[:,train_inds]\n",
    "y_train = y[0,train_inds]\n",
    "x_test = x[:, test_inds]\n",
    "y_test = y[0, test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_theta(lamda,w1,w2,dh,m,d):\n",
    "    if len(lamda) != 4 : return \"Lamda is error!\"\n",
    "    else:\n",
    "        w1=w1.reshape(1,dh*d)\n",
    "        w11 = np.sum(abs(w1))\n",
    "        w12 = np.linalg.det(w1*w1.T)\n",
    "        w2=w2.reshape(1,dh*m)\n",
    "        w21 = np.sum(abs(w2))\n",
    "        w22 = np.linalg.det(w2*w2.T)\n",
    "    W = [w11,w12,w21,w22]\n",
    "    return np.linalg.det(lamda*np.matrix(W).T)\n",
    "\n",
    "def train(x,y,w1,b1,w2,b2,num,lamda,dh,d,m,eta):\n",
    "    m, n = np.shape(x)\n",
    "    sum_loss = 0\n",
    "    Rmin = 10000\n",
    "    R = 0\n",
    "    itera = 0\n",
    "    while(itera < 100):\n",
    "        for i in range(int(n/num)):\n",
    "            k = np.arange(i*num,(i+1)*num)\n",
    "            sum_loss = fprop_k_loop(x,y,w1,b1,w2,b2,k)    \n",
    "        R = (1/(n/num))*sum_loss+L_theta(lamda,w1,w2,dh,m,d)\n",
    "        if R < Rmin:\n",
    "            Rmin = R\n",
    "            w1min = w1\n",
    "            w2min = w2\n",
    "            b1min = b1\n",
    "            b2min = b2\n",
    "            for i in range(int(n/num)):\n",
    "                k = np.arange(i*num,(i+1)*num)\n",
    "                grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1,grad_b1 = bprop_k_loop(m,x,y,w1,b1,w2,b2,k,dh)\n",
    "                w1 -= eta*grad_w1\n",
    "                b1 -= eta*grad_b1\n",
    "                w2 -= eta*grad_w2\n",
    "                b2 -= eta*grad_b2\n",
    "            itera += 1\n",
    "        else: \n",
    "            break\n",
    "    if itera == 10000 :\n",
    "        print(' not finish yet...')\n",
    "    return (Rmin, w1min, w2min, b1min, b2min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = np.matrix([0.0]*dh).T\n",
    "b2 = np.matrix([0.0]*dh).T\n",
    "Rmin, w1min, w2min, b1min, b2min = train(x_train,y_train,w1,b1,w2,b2,20,lamda,dh,d,m,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8184499918000228 [[-0.11729274  0.31165869]\n",
      " [-0.70208503 -0.27719362]] [[-0.15544452 -0.14049621]\n",
      " [-0.17804743 -0.14055413]] [[-0.00015906]\n",
      " [-0.00185901]] [[ 0.0129351]\n",
      " [-0.0129351]]\n"
     ]
    }
   ],
   "source": [
    "print(Rmin, w1min, w2min, b1min, b2min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. As a second step, copy your existing implementation to modify it to a new implementation that will use matrix calculus (instead of a loop) on batches of size K to improve efficiency. Take the matrix expressions in numpy derived in the first part, and adapt them for a minibatch of size K. Show in your report what you have modified (describe the former and new expressions with the shapes of each matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prol_k(x,k): #x is a matrix of 1 row, then prolong x to a matrix of k row, each row equal to x\n",
    "    result=[]\n",
    "    for i in range(k):\n",
    "        temp=[]\n",
    "        for j in range(np.shape(x)[1]):  \n",
    "            temp.append(x[0,j])\n",
    "        result.append(temp)\n",
    "    result=np.matrix(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "b1: [[-1.75631058e-07]\n",
      " [ 3.46704925e-05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05],\n",
       "        [-1.75631058e-07,  3.46704925e-05]])"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prol_k(hs.T,10) # hs.T  1*2 -->> 10*2 \n",
    "print(np.shape(b1.T)[1])\n",
    "print('b1:',b1)\n",
    "b1_m = prol_k(b1.T,10).T # b d*1 -->> d*k\n",
    "b1_m.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate x,ha,hs,oa,os,l,l_sum in matrix\n",
    "\n",
    "# fprop in part3.1\n",
    "'''\n",
    "def fprop(x,y,w1,b1,w2,b2):\n",
    "\n",
    "    ha = w1*x+b1\n",
    "    hs = rect(ha)\n",
    "    oa = w2*hs+b2\n",
    "    os = softmax(oa)\n",
    "    L = -np.log(os[int(y)])\n",
    "    return(ha,hs,oa,os,L)\n",
    "'''\n",
    "\n",
    "def fprop_k_m(x,y,w1,b1,w2,b2,k,i): # add k and i: k is the size of minibatch, and i is the index of starting point.\n",
    "    b1_m = prol_k(b1.T,k).T # change b1 from dh*1 to dh*k\n",
    "    ha = w1*x[:,i:i+k]+b1 # x from a vectot of d*1 change to a matrix of d*k, so b1 must change to dh*k to match\n",
    "    hs = rect(ha)\n",
    "    b2 = prol_k(b2.T,k).T #change b2 from m*1 to m*k\n",
    "    oa = w2*hs+b2 # hs from a vectot of dh*1 change to a matrix of dh*k, so b2 must change to m*k to match\n",
    "    os = softmax(oa)\n",
    "    lable = []  # lable and temp are used for choosing the value of os to calculate loss\n",
    "    temp = []\n",
    "    for j in range(k):\n",
    "        lable.append(int(y[0,i+j]))\n",
    "        temp.append(j)\n",
    "    L = -np.log(os[lable,temp])\n",
    "    L_sum = np.sum(L)/n\n",
    "\n",
    "    return(ha, hs, oa, os, L, L_sum)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the shapes of each matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, b1 is dh * k, w1 is dh * d, b2 is m * k, w2 is m * dh, ha and hs are dh * k, oa and os are m * k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ha: [[-0.16624397 -0.19824557 -0.24601582  0.16635281 -0.25053435  0.23020405\n",
      "  -0.16478758 -0.25627299  0.24394943  0.08339416]\n",
      " [-0.03686463  0.26330663  0.04083425 -0.27705736  0.19761423 -0.23473652\n",
      "  -0.03941345  0.08181374 -0.0343577  -0.27921174]] \n",
      "hs: [[0.         0.         0.         0.16635281 0.         0.23020405\n",
      "  0.         0.         0.24394943 0.08339416]\n",
      " [0.         0.26330663 0.04083425 0.         0.19761423 0.\n",
      "  0.         0.08181374 0.         0.        ]] \n",
      "oa: [[ 0.00049939 -0.00343811 -0.00011125 -0.00608446 -0.00245574 -0.00861154\n",
      "   0.00049939 -0.00072406 -0.00915555 -0.00280116]\n",
      " [-0.00049939 -0.02300026 -0.00398888  0.00142324 -0.01738651  0.0021612\n",
      "  -0.00049939 -0.00749078  0.00232006  0.00046444]] \n",
      "os: [[0.50024969 0.50489038 0.50096941 0.49812308 0.50373262 0.49730684\n",
      "  0.50024969 0.50169167 0.49713113 0.4991836 ]\n",
      " [0.49975031 0.49510962 0.49903059 0.50187692 0.49626738 0.50269316\n",
      "  0.49975031 0.49830833 0.50286887 0.5008164 ]] \n",
      "label: 1.0 \n",
      "L: [[0.69364669 0.68341394 0.69121025 0.69690808 0.68570966 0.69854806\n",
      "  0.69364669 0.68976954 0.69890145 0.69478131]] \n",
      "L_sum: 0.006296850607501441\n"
     ]
    }
   ],
   "source": [
    "ha,hs,oa,os,L,L_sum = fprop_k_m(x,y,w1,b1,w2,b2,10,1)\n",
    "print('ha:',ha,'\\nhs:',hs,'\\noa:',oa,'\\nos:',os,'\\nlabel:',y[0,0],'\\nL:',L,'\\nL_sum:',L_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bprop in part3.1\n",
    "'''\n",
    "def bprop(m,x,y,w1,b1,w2,b2):\n",
    "    ha = fprop(x,y,w1,b1,w2,b2)[0]\n",
    "    hs = fprop(x,y,w1,b1,w2,b2)[1]\n",
    "    os = fprop(x,y,w1,b1,w2,b2)[3]\n",
    "    grad_oa = os-onehot(m,y)\n",
    "    grad_w2 = grad_oa*hs.T\n",
    "    grad_b2 = grad_oa\n",
    "    grad_hs = w2.T*grad_oa\n",
    "    grad_ha=[]\n",
    "    for i in range(dh):\n",
    "        if (ha[i,0] <= 0) : grad_ha.append(0)\n",
    "        else: grad_ha.append(grad_hs[i,0])\n",
    "    grad_ha = np.matrix(grad_ha).T\n",
    "    grad_w1 = grad_ha*x.T \n",
    "    grad_b1 = grad_ha\n",
    "    return(grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1, grad_b1)\n",
    "'''\n",
    "def bprop_k_m(m,x,y,w1,b1,w2,b2,k,i,dh): # add k and i: k is the size of minibatch, and i is the index of starting point.\n",
    "    ha = fprop_k_m(x,y,w1,b1,w2,b2,k,i)[0]\n",
    "    hs = fprop_k_m(x,y,w1,b1,w2,b2,k,i)[1]\n",
    "    os = fprop_k_m(x,y,w1,b1,w2,b2,k,i)[3] \n",
    "    grad_oa = os-onehot(m,y[0,i:i+k])\n",
    "    grad_w2 = grad_oa*hs.T\n",
    "    grad_b2 = grad_oa\n",
    "    grad_hs = w2.T*grad_oa\n",
    "    grad_ha = []\n",
    "    for j in range(k):\n",
    "        temp = []\n",
    "        for m in range(dh):\n",
    "            if (ha[m,j] <= 0) : temp.append(0)\n",
    "            else: \n",
    "                temp.append(grad_hs[m,j])\n",
    "        grad_ha.append(temp)\n",
    "    grad_ha = np.matrix(grad_ha).T\n",
    "    grad_w1 = grad_ha*x[:,i:i+k].T \n",
    "    grad_b1 = grad_ha\n",
    "    return(np.sum(grad_oa,axis=1)/k, grad_w2, np.sum(grad_b2,axis=1)/k, np.sum(grad_hs,axis=1)/k, \n",
    "           np.sum(grad_ha,axis=1)/k, grad_w1, np.sum(grad_b1,axis=1)/k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the shapes of each matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, grad_b1 is dh * 1, grad_w1 is dh * d, grad_b2 is m * 1, grad_w2 is m * dh, \n",
    "# grad_ha and grad_hs are dh * 1, grad_oa and grad_os are m * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_oa:\n",
      " [[-0.5008164]\n",
      " [ 0.5008164]] \n",
      "grad_w2:\n",
      " [[-0.04176517 -0.        ]\n",
      " [ 0.04176517  0.        ]] \n",
      "grad_b2:\n",
      " [[-0.5008164]\n",
      " [ 0.5008164]] \n",
      "grad_w1:\n",
      " [[-0.01609751  0.0199175 ]\n",
      " [-0.          0.        ]] \n",
      "grad_b1:\n",
      " [[0.02560931]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1, grad_b1 = bprop_k_m(m,x,y,w1,b1,w2,b2,1,10)\n",
    "print('grad_oa:\\n',grad_oa,'\\ngrad_w2:\\n',grad_w2,'\\ngrad_b2:\\n',grad_b2,'\\ngrad_w1:\\n',grad_w1,'\\ngrad_b1:\\n',grad_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare both implementations (with a loop and with matrix calculus) to check that they both give the same values for the gradients on the parameters, first for K = 1, then for K = 10. Display the gradients for both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_oa:\n",
      " [[ 0.50191452]\n",
      " [-0.50191452]] \n",
      "grad_w2:\n",
      " [[ 0.          0.04740948]\n",
      " [ 0.         -0.04740948]] \n",
      "grad_b2:\n",
      " [[ 0.50191452]\n",
      " [-0.50191452]] \n",
      "grad_w1:\n",
      " [[0.         0.        ]\n",
      " [0.02594793 0.01131677]] \n",
      "grad_b1:\n",
      " [[0.        ]\n",
      " [0.03538547]]\n"
     ]
    }
   ],
   "source": [
    "# with a loop\n",
    "k=np.arange(1)\n",
    "grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1,grad_b1 \n",
    "= bprop_k_loop(m,x,y,w1,b1,w2,b2,k,dh)\n",
    "print('grad_oa:\\n',grad_oa,'\\ngrad_w2:\\n',grad_w2,'\\ngrad_b2:\\n',grad_b2,\n",
    "      '\\ngrad_w1:\\n',grad_w1,'\\ngrad_b1:\\n',grad_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_oa:\n",
      " [[ 0.50191452]\n",
      " [-0.50191452]] \n",
      "grad_w2:\n",
      " [[ 0.          0.04740948]\n",
      " [-0.         -0.04740948]] \n",
      "grad_b2:\n",
      " [[ 0.50191452]\n",
      " [-0.50191452]] \n",
      "grad_w1:\n",
      " [[0.         0.        ]\n",
      " [0.02594793 0.01131677]] \n",
      "grad_b1:\n",
      " [[0.        ]\n",
      " [0.03538547]]\n"
     ]
    }
   ],
   "source": [
    "# with matrix calculus\n",
    "grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1, grad_b1 \n",
    "= bprop_k_m(m,x,y,w1,b1,w2,b2,1,0)\n",
    "print('grad_oa:\\n',grad_oa,'\\ngrad_w2:\\n',grad_w2,'\\ngrad_b2:\\n',grad_b2,\n",
    "      '\\ngrad_w1:\\n',grad_w1,'\\ngrad_b1:\\n',grad_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_oa:\n",
      " [[-0.1993741]\n",
      " [ 0.1993741]] \n",
      "grad_w2:\n",
      " [[-0.03218852 -0.02421717]\n",
      " [ 0.03218852  0.02421717]] \n",
      "grad_b2:\n",
      " [[-0.1993741]\n",
      " [ 0.1993741]] \n",
      "grad_w1:\n",
      " [[-6.70019602e-03 -5.63076924e-05]\n",
      " [-9.86571300e-03 -1.98683298e-03]] \n",
      "grad_b1:\n",
      " [[ 0.00770831]\n",
      " [-0.0104821 ]]\n"
     ]
    }
   ],
   "source": [
    "# with a loop\n",
    "k=np.arange(10)\n",
    "grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1,grad_b1 \n",
    "= bprop_k_loop(m,x,y,w1,b1,w2,b2,k,dh)\n",
    "print('grad_oa:\\n',grad_oa,'\\ngrad_w2:\\n',grad_w2,'\\ngrad_b2:\\n',grad_b2,\n",
    "      '\\ngrad_w1:\\n',grad_w1,'\\ngrad_b1:\\n',grad_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_oa:\n",
      " [[-0.1993741]\n",
      " [ 0.1993741]] \n",
      "grad_w2:\n",
      " [[-0.32188522 -0.24217167]\n",
      " [ 0.32188522  0.24217167]] \n",
      "grad_b2:\n",
      " [[-0.1993741]\n",
      " [ 0.1993741]] \n",
      "grad_w1:\n",
      " [[-0.06700196 -0.00056308]\n",
      " [-0.09865713 -0.01986833]] \n",
      "grad_b1:\n",
      " [[ 0.00770831]\n",
      " [-0.0104821 ]]\n"
     ]
    }
   ],
   "source": [
    "# with matrix calculus\n",
    "grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1, grad_b1 \n",
    "= bprop_k_m(m,x,y,w1,b1,w2,b2,10,0)\n",
    "print('grad_oa:\\n',grad_oa,'\\ngrad_w2:\\n',grad_w2,'\\ngrad_b2:\\n',grad_b2,\n",
    "      '\\ngrad_w1:\\n',grad_w1,'\\ngrad_b1:\\n',grad_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time how long takes an epoch on fashion MNIST (1 epoch = 1 full traversal through the whole training set) for K = 100 for both versions (loop over a minibatch and matrix calculus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.mnist_reader as mnist_reader\n",
    "from utils import mnist_reader\n",
    "XM_train, YM_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "XM_test, YM_test = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[ 0.02645721 -0.03125051  0.01103898 ... -0.02781237  0.00446634\n",
      "   0.01281825]\n",
      " [-0.01978466  0.00860278  0.01146674 ...  0.02790571  0.014303\n",
      "   0.00416316]\n",
      " [ 0.02661945 -0.02637931 -0.01687022 ... -0.00319377  0.03562851\n",
      "   0.01669136]\n",
      " ...\n",
      " [ 0.01851909  0.02458761  0.01521257 ...  0.0355784   0.01752228\n",
      "   0.03024079]\n",
      " [ 0.02550762 -0.0141292   0.02581921 ... -0.01821978  0.0072019\n",
      "   0.01949766]\n",
      " [ 0.01845047 -0.00039195 -0.01958084 ...  0.03209723 -0.01975679\n",
      "   0.01429275]]\n",
      "w2:\n",
      " [[ 3.68380004e-03 -4.23990514e-03 -1.33587984e-03  1.46790578e-03\n",
      "   4.24069070e-03 -4.54284809e-03  3.20926730e-03  1.31413371e-03\n",
      "   5.41085061e-03 -2.96235425e-03 -4.50725994e-03 -4.59360179e-03\n",
      "   5.85360522e-03  9.71599497e-03 -9.28450804e-03 -7.85119126e-03\n",
      "   2.68005146e-03  9.80412704e-04 -9.05699188e-03 -2.24669076e-04\n",
      "  -8.71509463e-03 -7.25935391e-03  8.04167992e-03 -1.51156573e-03\n",
      "   2.09905922e-03 -1.94510304e-03  3.02757618e-03 -6.20199877e-04\n",
      "   5.41397954e-03  1.79552196e-03  5.68197493e-03  5.07883707e-03\n",
      "  -6.00938408e-03 -9.36131581e-03 -3.65686487e-04 -1.13288535e-03\n",
      "   3.88369128e-03 -6.03296765e-04  7.95092811e-03  7.92192368e-04\n",
      "   3.09966871e-03 -4.52727977e-03 -4.29554719e-04 -8.78861875e-03\n",
      "  -9.92001372e-03  1.16153109e-04 -6.28131967e-03  1.30655702e-03\n",
      "  -5.58486672e-03  5.75586552e-03]\n",
      " [-2.66724706e-03  9.90467824e-03  6.51871234e-03  8.09394723e-03\n",
      "   4.54554812e-03 -6.35289722e-03  4.59480145e-03  4.05290258e-03\n",
      "  -6.21205544e-03  7.01715965e-04  2.80539691e-03 -3.05155663e-03\n",
      "  -6.34218755e-03  1.86148477e-03 -6.36274276e-03 -6.99604461e-03\n",
      "  -8.23843617e-04  6.50128497e-04 -2.42826630e-03 -2.21499350e-03\n",
      "  -3.11189190e-03 -4.33183001e-03 -4.00873382e-03  2.59164211e-04\n",
      "  -1.74720607e-03  2.78184940e-03 -6.22443261e-03  8.03746046e-03\n",
      "  -4.60985134e-03 -9.25387009e-03 -7.32211645e-03  3.27089861e-04\n",
      "  -4.48957007e-03 -9.38290109e-03 -4.23386243e-03 -4.25313325e-03\n",
      "  -8.16000869e-03 -3.34400824e-03 -6.30916659e-03  7.66871004e-03\n",
      "   2.62000758e-03  7.73624989e-03  4.03359454e-04  9.46284405e-03\n",
      "   1.83249037e-03 -7.38037009e-03 -8.48443493e-04 -5.93115275e-03\n",
      "  -3.24678984e-03 -7.14079809e-03]\n",
      " [ 6.72511904e-03 -5.89312715e-03  8.09679157e-03  2.57960290e-04\n",
      "   5.37581015e-03  4.12183001e-03 -7.46384745e-03 -2.00623413e-03\n",
      "  -8.82658689e-03 -3.29091141e-03  2.57960450e-04  3.06921380e-03\n",
      "   7.97809074e-04 -4.27289780e-03  1.18635820e-03 -6.02165808e-03\n",
      "   1.02096304e-03 -3.48852128e-03  2.35177757e-03 -3.62098716e-03\n",
      "   5.04800973e-03 -1.82347009e-03 -1.98177382e-03 -5.94537084e-04\n",
      "  -2.40629529e-03 -9.95960814e-04  5.41229622e-03 -6.09430663e-03\n",
      "  -9.58237008e-03 -9.76911731e-03  5.90767711e-03  9.14580294e-03\n",
      "  -8.20836640e-03  9.31377540e-03 -3.15194057e-03  9.33987800e-03\n",
      "  -5.42151636e-03  1.60473152e-03 -7.81432559e-03  9.25160578e-03\n",
      "   5.24820456e-03  6.64186835e-03  8.10006836e-03  3.81140044e-03\n",
      "  -4.22464852e-03  6.17724929e-03 -4.69987730e-03  7.71306150e-03\n",
      "   7.35273365e-03  9.76395553e-03]\n",
      " [ 4.32724713e-03 -3.16396370e-03 -2.98748252e-03 -1.29450943e-03\n",
      "   6.08231250e-03 -7.27023139e-03  6.27609881e-03 -4.81347660e-03\n",
      "   5.97932236e-03 -8.81459493e-04 -1.77166953e-03 -5.13699632e-03\n",
      "   6.45102578e-03  9.31897484e-03  3.46488715e-03 -6.64506072e-03\n",
      "   3.29367886e-03 -2.22356620e-06  8.92795245e-03 -7.63044374e-03\n",
      "  -2.71293726e-03 -7.66853956e-03  3.70057123e-03  1.01696256e-03\n",
      "   4.01945055e-03 -7.86174152e-04 -9.74259669e-03  1.72069431e-04\n",
      "   6.24999184e-03  2.90520224e-03 -4.81373324e-03 -8.71087820e-03\n",
      "  -8.46325739e-03 -7.01793738e-03  8.04564363e-03  9.26920605e-03\n",
      "   8.10553874e-03 -8.70563275e-03 -5.70013038e-03 -3.32755776e-03\n",
      "   4.00555206e-03  8.63730592e-03 -5.83358662e-03  9.62618511e-03\n",
      "   9.73411636e-03  4.84304214e-03  1.90514646e-03 -3.17631790e-03\n",
      "   2.23615544e-03  5.66064323e-03]\n",
      " [-2.88835872e-04 -7.53652815e-03  9.67987306e-03 -6.51084628e-04\n",
      "  -5.23952885e-03  1.91395894e-03 -3.02066825e-03 -2.12146268e-03\n",
      "  -1.72326498e-03  7.35301175e-03  6.02552636e-03 -7.11689708e-04\n",
      "   6.13725944e-03 -1.14025531e-03 -3.36374823e-03  7.67199965e-03\n",
      "  -8.00319157e-03  7.35455801e-03 -8.02749424e-03  3.57629469e-03\n",
      "   7.00615572e-03 -3.14376140e-03 -3.89636061e-03 -5.10808040e-03\n",
      "   4.51630970e-03 -6.95511914e-03 -9.71691474e-03  8.01924793e-04\n",
      "   2.94002918e-03  4.42818290e-03  3.91131232e-03  2.31425033e-03\n",
      "   9.47503329e-03 -2.19278361e-03 -8.48797884e-03  5.26545015e-03\n",
      "   7.86612353e-03 -3.51235152e-04 -8.24961569e-03 -7.63681761e-03\n",
      "   8.30320106e-03  4.66134981e-03  6.96970671e-03  1.24620360e-03\n",
      "   8.94976792e-03 -7.62389606e-03  8.01100926e-03 -2.65903102e-04\n",
      "  -2.50321617e-03  2.14278157e-03]\n",
      " [-1.50424338e-03  7.06264532e-03  4.95771338e-03  7.87044961e-03\n",
      "   7.53668712e-03  8.08691048e-03  4.87527746e-03 -1.11061780e-03\n",
      "  -6.91321344e-03 -6.64209528e-03  1.01738133e-03 -2.11408703e-03\n",
      "   8.64044197e-03 -7.95667138e-03  2.21536041e-03  8.96958484e-04\n",
      "  -2.96054511e-04  6.33628486e-03  7.26537833e-03 -5.05113823e-03\n",
      "   2.58055365e-03 -5.73757468e-03 -1.55979184e-03 -7.59969587e-04\n",
      "  -9.70340224e-03  4.76334534e-03  7.48156272e-03  9.54776703e-03\n",
      "  -8.13388485e-04 -5.94161767e-03  9.21807749e-03 -8.20393714e-03\n",
      "  -5.65730864e-03  6.51599535e-03  4.89865051e-03  7.45186498e-03\n",
      "  -4.15783057e-04 -6.01497448e-03 -1.34879042e-03 -3.44639909e-03\n",
      "  -4.12924511e-03  5.11805573e-03 -1.75235756e-03  2.40334454e-03\n",
      "   6.27143913e-04 -8.77457377e-03 -2.12992111e-03 -9.78817834e-03\n",
      "   3.13540874e-03 -5.68987023e-03]\n",
      " [ 7.20555288e-03  2.94725678e-03 -2.68892814e-03 -1.18551278e-03\n",
      "  -2.30137281e-03 -5.13683696e-03 -5.36393130e-03  9.71049403e-03\n",
      "   9.48515626e-03  3.51501108e-03 -9.93110228e-03  8.25234038e-03\n",
      "  -8.79610072e-03  8.93926400e-03  7.22686973e-03 -5.65184333e-03\n",
      "  -4.94733346e-03 -8.16258629e-03  8.67185575e-03  9.80055207e-03\n",
      "   2.44623622e-03 -3.06932422e-04  6.96253525e-03 -7.95261009e-03\n",
      "  -5.69169133e-03 -8.62027421e-03 -1.46874428e-03  7.27949583e-03\n",
      "   2.69523380e-04 -9.29530396e-03  8.34729450e-03  4.89412305e-03\n",
      "   4.01719395e-03  9.32806527e-03 -4.14848719e-03 -7.80979068e-03\n",
      "   4.43259121e-03 -9.58170644e-03 -8.63194976e-03  8.74335161e-03\n",
      "   7.14454862e-03 -9.65060886e-03 -7.49708211e-03  8.01384050e-03\n",
      "   8.32290959e-04 -1.18704439e-03  2.14088394e-03 -4.42547507e-03\n",
      "  -7.96663904e-03  8.69005299e-03]\n",
      " [ 8.65197225e-03 -8.09123721e-03  8.94778211e-03  4.37196366e-03\n",
      "  -5.48205994e-04  9.13928188e-03 -8.39102271e-03  8.27873756e-03\n",
      "   3.51071363e-03  4.86842006e-03 -8.25826247e-03 -1.74979004e-03\n",
      "  -5.89830399e-03  6.20126836e-03  7.87428688e-03 -7.79753650e-03\n",
      "   9.88724409e-03 -8.25787542e-03 -3.05340333e-03  7.89423896e-03\n",
      "  -2.09333637e-03 -8.56078095e-03  5.49328964e-04  2.70677608e-03\n",
      "   9.52852817e-03  4.56371164e-03  9.28657998e-03  1.83045980e-03\n",
      "  -7.73257489e-03 -9.99134988e-03 -8.28952897e-04 -5.75431632e-04\n",
      "  -5.23420718e-03 -4.15201505e-04 -1.05489864e-03 -3.84974896e-03\n",
      "  -3.34000413e-03  1.40427993e-03 -5.77252392e-03 -3.41861008e-03\n",
      "   5.13481118e-04 -5.24739008e-03 -3.23509167e-03 -4.64440302e-03\n",
      "  -8.87420885e-03  2.72546307e-03 -9.26008169e-03  4.72513873e-03\n",
      "   4.37622967e-03  3.93315345e-03]\n",
      " [-2.78946924e-03 -6.78793203e-03  4.44178054e-03 -1.67062038e-03\n",
      "  -5.61086144e-03  4.55845170e-03 -2.50424968e-03  7.53791005e-03\n",
      "   8.76384909e-04 -3.61714368e-03 -3.63904222e-03 -2.48776115e-03\n",
      "   2.94601150e-03  7.88768793e-04  2.68966056e-05 -8.90914726e-03\n",
      "  -1.28610455e-03 -7.38668566e-03 -6.18339193e-04 -8.45617346e-03\n",
      "   2.33282576e-03 -4.92327618e-04  5.62258506e-04  8.83902085e-03\n",
      "  -7.27643883e-03 -9.98082745e-03 -6.29031926e-03  5.57163952e-03\n",
      "   3.55534143e-03 -1.76392775e-04  9.39432721e-04 -6.16881613e-03\n",
      "  -2.04245313e-03 -1.77138311e-03  8.08233477e-03 -3.87273602e-03\n",
      "   9.43770185e-04  9.71454785e-03 -3.89804826e-03 -1.56425746e-03\n",
      "  -8.13145446e-03 -1.55204456e-03 -8.81418947e-03 -6.95000057e-03\n",
      "   2.36872484e-03  7.24900119e-03 -1.17232827e-03  4.88256180e-03\n",
      "  -1.30412975e-04  2.66949215e-03]\n",
      " [-7.97914833e-03 -8.41409394e-03  8.29568010e-03 -1.74212949e-04\n",
      "  -1.92603279e-04 -3.69995498e-03  4.78013698e-03 -5.50655032e-03\n",
      "   4.34311728e-03 -4.08349688e-03 -7.64878631e-03  3.65548050e-05\n",
      "   6.10297218e-03  5.84610155e-03  3.88149089e-03  4.25686541e-03\n",
      "   4.41252175e-04  8.87252080e-03 -2.20709946e-03  8.72548316e-03\n",
      "   3.44296963e-05  8.75664745e-03 -8.74484167e-03 -8.83174628e-03\n",
      "   9.29383939e-05  8.42656107e-04  4.14750254e-03  8.53692187e-03\n",
      "   9.70605814e-03 -5.12119843e-03 -5.15560281e-03  2.64228134e-03\n",
      "  -9.32433072e-03 -2.64567227e-03 -2.59173422e-03  5.92611869e-03\n",
      "  -8.08504345e-03 -9.05457573e-03  1.42198949e-04  3.05733270e-03\n",
      "   6.71054829e-03 -4.52783768e-03 -5.53098460e-03 -3.30952592e-04\n",
      "   9.24853244e-03  7.75850257e-03 -2.65086336e-03 -1.44888871e-03\n",
      "   6.63443921e-03 -2.41408589e-03]]\n"
     ]
    }
   ],
   "source": [
    "m = 10\n",
    "dh = 50\n",
    "train_data = np.matrix(XM_train).T\n",
    "label = np.matrix(YM_train)\n",
    "d = np.shape(train_data)[0]\n",
    "w1 = np.matrix((np.random.uniform(-1/d**0.5,1/d**0.5,d*dh)).reshape(dh,d))\n",
    "print('w1:\\n',w1)\n",
    "b1 = np.matrix([0]*dh).T\n",
    "w2 = np.matrix((np.random.uniform(-1/dh*0.5,1/dh*0.5,dh*m)).reshape(m,dh))\n",
    "print('w2:\\n',w2)\n",
    "b2 = np.matrix([0]*m).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def mnist_loop_all(x, y, w1, b1, w2, b2, dh, m):\n",
    "    start = time.time()\n",
    "    num = int(np.shape(XM_train)[1]/100)\n",
    "    for i in range(num):\n",
    "        k = np.arange(i*100, (i+1)*100)\n",
    "        fprop_k_loop(x,y,w1,b1,w2,b2,k)\n",
    "        goa_m, gw2_m, gb2_m, ghs_m, gha_m, gw1_m,gb1_m = bprop_k_loop(m,x,y,w1,b1,w2,b2,k,dh)\n",
    "    end = time.time()\n",
    "    running_time = end-start\n",
    "    return running_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for loopint all the data by minibatch: 0.6707990169525146\n"
     ]
    }
   ],
   "source": [
    "print('The time for looping all the data by minibatch:',\n",
    "      mnist_loop_all(train_data, label, w1, b1, w2, b2, dh, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_matrix_all(x, y, w1, b1, w2, b2, dh, m):\n",
    "    start = time.time()\n",
    "    num = int(np.shape(XM_train)[1]/100)\n",
    "    for i in range(num):\n",
    "        fprop_k_m(x,y,w1,b1,w2,b2,100,i*100)\n",
    "        bprop_k_m(m,x,y,w1,b1,w2,b2,100,i*100)\n",
    "    end = time.time()\n",
    "    running_time = end-start\n",
    "    return running_time    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for using matrix to calculate all the data by minibatch: 0.6026718616485596\n"
     ]
    }
   ],
   "source": [
    "print('The time for using matrix to calculate all the data by minibatch:',\n",
    "      mnist_loop_all(train_data, label, w1, b1, w2, b2, dh, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Adapt your code to compute the error (proportion of misclassified examples) on the training set as well as the total loss on the training set during each epoch of the training procedure, and at the end of each epoch, it computes the error and average loss on the validation set and the test set. Display the 6 corresponding figures (error and average loss on train/valid/test), and write them in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open('cercles.txt','r'))\n",
    "n = np.shape(data)[0]\n",
    "inds = np.arange(n)\n",
    "np.random.shuffle(inds)\n",
    "train_inds = inds[:700]\n",
    "valid_inds = inds[700:900]\n",
    "test_inds = inds[900:1100]\n",
    "x_train = np.matrix(data[train_inds,:-1]).T\n",
    "y_train = np.matrix(data[train_inds,-1])\n",
    "x_valid = np.matrix(data[valid_inds,:-1]).T\n",
    "y_valid = np.matrix(data[valid_inds,-1])\n",
    "x_test = np.matrix(data[test_inds,:-1]).T\n",
    "y_test = np.matrix(data[test_inds,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = 2\n",
    "d = 2\n",
    "m = 2\n",
    "np.random.seed(1)\n",
    "w1 = np.matrix((np.random.uniform(-1/d**0.5,1/d**0.5,d*dh)).reshape(dh,d))\n",
    "b1 = np.matrix([0.0]*dh).T\n",
    "w2 = np.matrix((np.random.uniform(-1/dh*0.5,1/dh*0.5,dh*m)).reshape(m,dh))\n",
    "b2 = np.matrix([0.0]*m).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we choose matrix calculate, because it's faster than looping\n",
    "# modifier fprop to add error\n",
    "def fprop_error(x,y,w1,b1,w2,b2,k,i): # add k and i: k is the size of minibatch, and i is the index of starting point.\n",
    "    b1_m = prol_k(b1.T,k).T # change b1 from dh*1 to dh*k\n",
    "    ha = w1*x[:,i:i+k]+b1 # x from a vectot of d*1 change to a matrix of d*k, so b1 must change to dh*k to match\n",
    "    hs = rect(ha)\n",
    "    b2 = prol_k(b2.T,k).T #change b2 from m*1 to m*k\n",
    "    oa = w2*hs+b2 # hs from a vectot of dh*1 change to a matrix of dh*k, so b2 must change to m*k to match\n",
    "    os = softmax(oa)\n",
    "    err = 0\n",
    "    for j in range(k):\n",
    "        if (os[0,j] >= os[1,j]):\n",
    "            pre_y = 0\n",
    "        else:\n",
    "            pre_y = 1\n",
    "        if pre_y != y[0,i+j]:\n",
    "            err += 1\n",
    "    lable = []  # lable and temp are used for choosing the value of os to calculate loss\n",
    "    temp = []\n",
    "    for j in range(k):\n",
    "        lable.append(int(y[0,i+j]))\n",
    "        temp.append(j)\n",
    "    L = -np.log(os[lable,temp])\n",
    "    L_sum = np.sum(L)/k\n",
    "    return(ha, hs, oa, os, L, L_sum,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_theta(lamda,w1,w2,dh,m,d):\n",
    "    if len(lamda) != 4 : return \"Lamda is error!\"\n",
    "    else:\n",
    "        w1=w1.reshape(1,dh*d)\n",
    "        w11 = np.sum(abs(w1))\n",
    "        w12 = np.linalg.det(w1*w1.T)\n",
    "        w2=w2.reshape(1,dh*m)\n",
    "        w21 = np.sum(abs(w2))\n",
    "        w22 = np.linalg.det(w2*w2.T)\n",
    "    W = [w11,w12,w21,w22]\n",
    "    return np.linalg.det(lamda*np.matrix(W).T)\n",
    "\n",
    "def train(x,y,w1,b1,w2,b2,k,lamda,dh,d,m,eta,epsilon): # k is size of minibatch, eta is hyperparameter\n",
    "    m, n = np.shape(x)\n",
    "    sum_loss = 0\n",
    "    err_sum = 0\n",
    "    Rmin = 10000\n",
    "    R = 0\n",
    "    itera = 0\n",
    "    err_prop = 0\n",
    "    grad_b2 = 1\n",
    "    num = int(n/k)\n",
    "    while((itera < 1000 )& (np.sum(grad_b2) > epsilon)):\n",
    "        for i in range(num):\n",
    "            ha, hs, oa, os, L, loss, err = fprop_error(x,y,w1,b1,w2,b2,k,i*k)\n",
    "            sum_loss += loss*k\n",
    "            err_sum += err   \n",
    "        R = (1/n)*sum_loss+L_theta(lamda,w1,w2,dh,m,d)\n",
    "        if R < Rmin:\n",
    "            Rmin = R\n",
    "            w1min = w1\n",
    "            w2min = w2\n",
    "            b1min = b1\n",
    "            b2min = b2\n",
    "            sum_loss_result = sum_loss\n",
    "            err_result = err_sum\n",
    "            for i in range(num):\n",
    "                grad_oa, grad_w2, grad_b2, grad_hs, grad_ha, grad_w1, grad_b1 = bprop_k_m(m,x,y,w1,b1,w2,b2,k,i*k)\n",
    "                w1 -= eta*grad_w1\n",
    "                b1 -= eta*grad_b1\n",
    "                w2 -= eta*grad_w2\n",
    "                b2 -= eta*grad_b2\n",
    "            itera += 1\n",
    "        else: \n",
    "            break\n",
    "    if itera == 1000 :\n",
    "        print(' not finish yet...')\n",
    "    return (Rmin, w1min, w2min, b1min, b2min, sum_loss_result/n, err_result/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rmin, w1min, w2min, b1min, b2min, loss_avg, err_prop = train(x_train,y_train,w1,b1,w2,b2,100,[0.01,0.01,0.01,0.01],dh,d,m,0.001,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.694757623537982\n",
      "0.5042857142857143\n"
     ]
    }
   ],
   "source": [
    "print(loss_avg)\n",
    "print(err_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6945979647817484\n",
      "0.53\n"
     ]
    }
   ],
   "source": [
    "Rmin, w1min, w2min, b1min, b2min, loss_avg, err_prop = train(x_valid,y_valid,w1,b1,w2,b2,100,[0.01,0.01,0.01,0.01],dh,d,m,0.001,0.001)\n",
    "print(loss_avg)\n",
    "print(err_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6934102248402416\n",
      "0.48\n"
     ]
    }
   ],
   "source": [
    "Rmin, w1min, w2min, b1min, b2min, loss_avg, err_prop = train(x_test,y_test,w1,b1,w2,b2,100,[0.01,0.01,0.01,0.01],dh,d,m,0.001,0.001)\n",
    "print(loss_avg)\n",
    "print(err_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
